#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
GrabCut and Semantic Segmentation
\end_layout

\begin_layout Author
Tejas Sharma, Jiajun Xu, Youxin Chen, Jacob Irle
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
The goal of our project was to implement Grabcut, which is a foreground
 extraction algorithm, and then to further utilize this tool in Semantic
 Segmentation, which, when trained with a set of pre-labeled images, can
 complete GrabCut without any human interaction at all for that class of
 objects.
 Our motivation for this project was to be able to quickly detect foreground
 in images with minimal user interaction.
\end_layout

\begin_layout Section
GrabCut
\end_layout

\begin_layout Standard
This part of the project was completed by Tejas Sharma and Jiajun Xu
\end_layout

\begin_layout Subsection
Outside Resources
\end_layout

\begin_layout Itemize
Our grabcut implementation is based on the paper “ ‘GrabCut’ -- Interactive
 Foreground Extraction using Iterated Graph Cuts” by Carsten Rother, Vladimir
 Kolmogorov, and Andrew Blake.
\end_layout

\begin_layout Itemize
GrabCut is based upon the paper “Graph Cut”, by Yuri Boykov and Marie-Pierre
 Jolly.
\end_layout

\begin_layout Itemize
The paper “Implementing GrabCut” by Justin Talbot and Xiaoqian Xu was also
 crucial in our implementation, as it explained the method in a much simpler
 manner, which increased our understanding significantly.
\end_layout

\begin_layout Itemize
We also used the “PyMaxFlow” library, which is a library for solving min-cut/max
-flow in python.
 This library is based on the method described in “An Experimental Comparison
 of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision” by Yuri
 Boykov and Vladimir Kolmogorov.
 We initially created our own implementation based on the same paper, but
 found that it was too slow to be practically used due to being written
 in pure python.
 PyMaxFlow creates a wrapper for the C++ version, which makes it a lot faster.
\end_layout

\begin_layout Itemize
Our UI is heavily based upon the Python example available in the OpenCV
 github repo.
 
\end_layout

\begin_layout Subsection
Implementation Details
\end_layout

\begin_layout Standard
GrabCut is based upon the Graph Cut algorithm, which is also used for image
 segmentation.
 The key difference between the two is that Graph Cut could only be used
 in black and white images, and required a lot more user interaction.
\end_layout

\begin_layout Standard
It uses iterative learning, which greatly reduces the amount of user interaction
, but still allows the user to hardcode foreground and background pixels.
 At a high level, GrabCut attempts to minimize the energy between pixels
 in an image in order to find an optimal bounding cut.
 It iteratively improves this cut by recalculating the energy based on its
 previous classification until convergence.
\end_layout

\begin_layout Subsubsection
Gaussian Mixture Models
\end_layout

\begin_layout Standard
GrabCut represents the foreground and the background of an image with 2
 Gaussian Mixture Models, each of which has K gaussian components.
 Every pixel of the image belongs to one of the foreground or background
 component.
 Each gaussian component stores the mean 
\begin_inset Formula $(\mu)$
\end_inset

, the covariance matrix 
\begin_inset Formula $(\Sigma)$
\end_inset

 and a weight 
\begin_inset Formula $(\pi)$
\end_inset

.
 We use these gaussian components to calculate the likelihood that the pixel
 belongs to either the foreground or the background 
\end_layout

\begin_layout Standard
We calculate the likelihood a pixel belongs to a GMM using equation 2 from
 “Implementing GrabCut
\begin_inset Quotes erd
\end_inset

, which gives us a likelihood as the function of a pixel 
\begin_inset Formula $m$
\end_inset


\begin_inset Formula 
\[
D(m)=-\log\sum_{i=1}^{K}\left[\pi(\alpha_{m},i)\frac{1}{\sqrt{\textrm{det}\Sigma(\alpha_{m},i)}}\times\exp\left(\frac{1}{2}\left[z_{m}-\mu(\alpha_{m},i)\right]^{T}\Sigma(\alpha_{m},i)^{-1}\left[z_{m}-\mu(\alpha_{m},i)\right]\right)\right]
\]

\end_inset

where 
\begin_inset Formula $z_{m}$
\end_inset

is the colour of the pixel, and 
\begin_inset Formula $(\alpha_{m},i)$
\end_inset

 essentially allows us to uniquely identify the cluster covariance and mean
 that we are trying to access.
\end_layout

\begin_layout Subsubsection
Pixel Classifications
\end_layout

\begin_layout Standard
When creating the GMMs, we use the initial user input, which is a simple
 rectangle that is drawn around the foreground object, to create a set of
 classifications for the pixels.
 In “Implementing GrabCut”, they use a trimap of Foreground, Background
 and Unknown.
 However, when looking at the example code for our UI, we noticed that it
 uses four values: Background, Foreground, Potential Background, and Potential
 Foreground.
 We decided this made more sense to us, and would make implementation easier,
 so we also used this method.
 In the initial step, everything outside the rectangle is classified as
 Background, and everything inside it is classified as Potential Foreground.
 To note the difference in this report between the classification Foreground
 and the regular word foreground, we will be capitalising the classifications.
\end_layout

\begin_layout Subsubsection
Graph Cut
\end_layout

\begin_layout Standard
To create the graph used in Graph Cut, we represent each pixel as a node
 in the graph.
 Each pixel is connected to its 8 adjacent pixels in the image (unless it
 is on one or more of the borders).
 We also add two terminal nodes for performing the max flow segmentation,
 one representing the source (background), and one representing the sink
 (foreground).
 The terminal nodes are connected to every pixel.
\end_layout

\begin_layout Standard
There are two types of edges in the graph: T-links, which are between a
 pixel and the two terminal nodes; and N-links, which are between a pixel
 and its 8 neighbours.
\end_layout

\begin_layout Standard
We build the N-links as follows:
\end_layout

\begin_layout Itemize
Each pixel is connected to its neighbours with the weight function 
\begin_inset Formula 
\[
N(m,n)=\frac{1}{dist(m,n)}e^{-\beta\Vert z_{m}-z_{n}\Vert^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta=\left(2\left\langle \left(z_{m}-z_{n}\right)^{2}\right\rangle \right)^{-1}$
\end_inset

, where 
\begin_inset Formula $\langle\rangle$
\end_inset

 represents an expectation over an image sample.
\end_layout

\begin_layout Itemize
The 
\begin_inset Formula $dist$
\end_inset

 factor simply denotes whether the pixels are diagonal neighbours or immediate
 neighbours as noted in 
\begin_inset Quotes eld
\end_inset

Implementing GrabCut
\begin_inset Quotes erd
\end_inset

, since the method in the original GrabCut paper favoured diagonal edges.
\end_layout

\begin_layout Standard
We build the T-links between the terminal nodes and the pixels by doing
 the following:
\end_layout

\begin_layout Itemize
First, we store the maximum weight created in the N-links.
 This will be used when a pixel has been clasified as Foreground or Background.
\end_layout

\begin_layout Itemize
If a pixel is classified as Foreground, we assign its foreground T-link
 to be the maximum weight, and its background T-link to be 0
\end_layout

\begin_layout Itemize
If a pixel is classified as Background, we assign its foreground T-link
 to be 0, and its background T-link to be the maximum weight
\end_layout

\begin_layout Itemize
If a pixel is classified as Potential Background or Potentail Foreground,
 we use the 
\begin_inset Formula $D(m)$
\end_inset

 function referenced earlier and assign its foreground T-link to be 
\begin_inset Formula $D_{bgd}(m)$
\end_inset

 and its background T-link to be 
\begin_inset Formula $D_{fgd}(m)$
\end_inset

.
\end_layout

\begin_layout Standard
After building the links, we run a max cut algorithm on it, and use the
 results to tell us which parts are foreground and background.
 Pixels that have been marked as Foreground or Background by the user are
 unchanged, and pixels that are not are updated to Potential Foreground
 and Potential Background based on the cut.
 These values are then fed back into the GMMs to iteratively run until convergen
ce.
\end_layout

\begin_layout Subsection
GrabCut Results
\end_layout

\begin_layout Standard
The following are the results of running our GrabCut implementation on some
 images that were used in the original papers.
\end_layout

\begin_layout Standard
The first is an image of a llama, which we decided worked pretty well without
 any secondary user input.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Original Image
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
GrabCut Output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Downloads/25394090_842509615910821_923845087_n.png
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Downloads/25360839_842509612577488_1812808336_n.png
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Llama Segmentation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
The next is an image of Lena Söderberg, which allowed us to show off the
 ability for users to mark certain areas as Foreground or Background that
 were incorrectly classified by GrabCut.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Original Image
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Initial GrabCut Output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/lena_rectangle.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/lena_rectangle_result.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
User Touchup
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Final GrabCut Output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/lena_touchup.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/lena_touchup_result.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Lena Segmentation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
Finally, we have an image of a soldier.
 We were expecting this image to be really hard due to the similarity of
 colours between the foreground and background.
 Indeed, the initial result was not great.
 However, even with such similar colours, after some user input we had a
 pretty good result.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Original Image
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Initial GrabCut Output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/soldier_rectangle.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/soldier_rectangle_result.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
User Touchup
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Final GrabCut Output
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/soldier_touchup.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/soldier_touchup_result.PNG
	scale 50

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Lena Segmentation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Semantic Segmentation
\end_layout

\begin_layout Standard
This part of the project was completed by Youxin Chen and Jacob Irle
\end_layout

\begin_layout Subsection
Outside Resources
\end_layout

\begin_layout Itemize
Our Semantic Segmentation is based on 
\begin_inset Quotes eld
\end_inset

Semantic Segmentation using GrabCut
\begin_inset Quotes erd
\end_inset

 by Christopher Göring, Björn Fröhlich, and Joachim Denzler.
\end_layout

\begin_layout Itemize
The metric we use to decide the similarity between two patches is Hu Moments,
 based on the paper 
\begin_inset Quotes eld
\end_inset

Visual Pattern Recognition by Moment Invariants
\begin_inset Quotes erd
\end_inset

 by Ming-Kuei Hu.
\end_layout

\begin_layout Subsection
Implementation Details
\end_layout

\begin_layout Standard
Semantic Segmentation uses GrabCut in order to complete GrabCut without
 any user interaction.
 It does this by essentially training a pair of Gaussian Mixture Models
 with a set of sample images of a particular class of object (e.g.
 cars, flowers), and then using those models in the GrabCut algorithm to
 get the foreground.
\end_layout

\begin_layout Subsubsection
Training the GMMs
\end_layout

\begin_layout Standard
The assumption that we make while implementing Semantic Segmentation is
 that all objects of a particulat class can be modeled using Gaussian Mixture
 Models.
 So, in order to bootstrap the algorithm, we manually label the background
 and foreground of some images of certain classes.
 A simplified training set can be seen below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/training_set.PNG

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Simplified Training Set
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We use photoshop to generate the corresponding foreground and background
 images.
 Those pixtures are saved as png files, with the irrelevant section having
 zero opacity.
\end_layout

\begin_layout Standard
We then load all the pictures, and only use pixels with an opacity value
 greater than 200.
\end_layout

\begin_layout Standard
After doing this, we feed all foreground pixels of a class to one GMM, and
 the background pixels to another GMM.
 After creating the GMMs, we finetune the number of components down to 5.
 We first train a series of GMMs with a number of different components,
 and run those GMMs on a validation of the same class.
\end_layout

\begin_layout Subsubsection
Creating a Segmentation
\end_layout

\begin_layout Standard
With the GMMs trained, we feed our test images to them, and label them as
 either foreground or background based on the following formula
\begin_inset Formula 
\[
\alpha_{i}^{*}=\underbrace{argmax}_{\alpha_{i}\in\left\{ \textrm{fgd,bgd}\right\} }p\left(z_{i}\vert\alpha_{i},k_{i},\theta\right),\forall i\in\left\{ 1,\ldots,N\right\} 
\]

\end_inset

Where 
\begin_inset Formula $N$
\end_inset

 is the number of pixels, 
\begin_inset Formula $z_{i}$
\end_inset

 is the i-th pixel, and 
\begin_inset Formula $\theta$
\end_inset

represents the GMMS we trained for each class.
\end_layout

\begin_layout Standard
With this, we get an initial segmentation, which together with the GMMs
 and GrabCut, can be used to optimize the segmentation that will later be
 used for classification.
\end_layout

\begin_layout Subsubsection
Classification
\end_layout

\begin_layout Standard
After GrabCut returns the optimized segmentation, we find the class which
 has the minimum distance to the segment.
 This is implemented by taking the distance between the Hu-moments of the
 segment and a given training set of images.
 The function shown below describes how we calculate the distance of a segment
 to a given class.
\begin_inset Formula 
\[
dist_{h}(\alpha_{c},Z_{c})=\underset{i=1\ldots N_{c}}{min}M(h(\alpha_{c}),h(\alpha^{i,c}))
\]

\end_inset


\end_layout

\begin_layout Standard
In order for this function to make sense, we must have the Hu moments of
 the training images and the segment.
 We can obtain the Hu moments of an image by first finding the central moments
 of an image.
 The central moments of an image are defined below, where 
\begin_inset Formula $M_{xx}$
\end_inset

 describes an already calculated moment
\begin_inset Formula 
\begin{align*}
\mu_{00} & =M_{00}\\
\mu_{01} & =0\\
\mu_{10} & =0\\
\mu_{11} & =M_{11}-\overline{x}M_{01}=M_{11}-\overline{y}M_{10}\\
\mu_{20} & =M_{20}-\overline{x}M_{10}\\
\mu_{02} & =M_{02}-\overline{x}M_{01}\\
\mu_{21} & =M_{21}-2\overline{x}M_{11}-\overline{y}M_{20}+2\overline{x}^{2}M_{01}\\
\mu_{12} & =M_{12}-2\overline{y}M_{11}-\overline{x}M_{02}+2\overline{y}^{2}M_{10}\\
\mu_{30} & =M_{30}-3\overline{x}M_{20}+2\overline{x}^{2}M_{10}\\
\mu_{03} & =M_{03}-3\overline{7}M_{02}+2\overline{y}^{2}M_{01}
\end{align*}

\end_inset

After the central moments are found, we can use them to calculate the Hu
 moments which are used in our distance calculation.
 There are 7 invariant Hu moments which we will use in order to calculate
 the distance.
\begin_inset Formula 
\begin{align*}
I_{1} & =\eta_{20}+\eta_{02}\\
I_{2} & =(\eta_{20}-\eta_{02})^{^{2}}+4\eta_{11}^{2}\\
I_{3} & =(\eta_{30}-3\eta_{12})^{^{2}}+(3\eta_{21}-\eta_{03})^{^{2}}\\
I_{4} & =(\eta_{30}+\eta_{12})^{^{2}}+(\eta_{21}+\eta_{03})^{^{2}}\\
I_{5} & =(\eta_{30}-3\eta_{12})(\eta_{30}+\eta_{12})\left[(\eta_{30}+\eta_{12})^{^{2}}-3(\eta_{21}-\eta_{03})^{^{2}}\right]+(3\eta_{21}-\eta_{03})(\eta_{21}+\eta_{03})\left[3(\eta_{30}+\eta_{12})^{^{2}}-(\eta_{21}+\eta_{03})^{^{2}}\right]\\
I_{6} & =(\eta_{20}-\eta_{02})\left[(\eta_{30}+\eta_{12})^{^{2}}-3(\eta_{21}+\eta_{03})^{^{2}}\right]+4\eta_{11}(\eta_{30}+\eta_{12})(\eta_{21}+\eta_{03})\\
I_{7} & =(3\eta_{21}-\eta_{03})(\eta_{30}+\eta_{12})\left[(\eta_{30}+\eta_{12})^{^{2}}-3(\eta_{21}-\eta_{03})^{^{2}}\right]-(\eta_{30}-3\eta_{12})(\eta_{21}+\eta_{03})\left[3(\eta_{30}+\eta_{12})^{^{2}}-(\eta_{21}+\eta_{03})^{^{2}}\right]
\end{align*}

\end_inset

After these are obtained we can then calculate the distance between two
 sets of hu moments as such:
\begin_inset Formula 
\[
M(H,H')=\sum_{i=1,\ldots,7}\left|\frac{\textrm{sign}(H_{i})}{\log|H_{i}|}-\frac{\textrm{sign}(H'_{i})}{\log|H'_{i}|}\right|
\]

\end_inset

Now that we have a way to calculate the distance between two sets of hu
 moments, we cans ee that the distance between a segment and a given class
 is described as the hu moment distance between the segment and a given
 class, which allows us to determine which class the segment belongs to
 by finding the class that gives us a minimum distance from the segment.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Subsubsection
Creating a segmentation
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/semanticsegmentation_car.PNG

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Segmentation on Car (Initial Image / After Initial Segmentation
 / After GrabCut Optimization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/semanticsegmentation_flower.PNG

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Segmentation on Flower (Initial Image / After Initial Segmentation
 / After GrabCut Optimization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Classification
\end_layout

\begin_layout Standard
For the below image, the results were:
\end_layout

\begin_layout Itemize
Distance to Car class: 0.0256119028253
\end_layout

\begin_layout Itemize
Distance to Flower class: 0.0311897888659 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/car_class.PNG

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Car Classification
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the below image, the results were:
\end_layout

\begin_layout Itemize
Distance to Car class: 0.00651206406006
\end_layout

\begin_layout Itemize
Distance to Flower class: 0.00531785182462 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename C:/Users/tejas/Pictures/flower_class.PNG

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Flower Classification
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As you can see, the classifying functionality of C-Grabcut works as intended,
 although the difference between the two distances is not as extreme as
 we initially thought it would be.
 This could be a result of our training set being fairly small for each
 class (4 images), which was due to the fact that we needed to manually
 construct the background and foreground images for each class.
 If we were able to automate that process, we may have been able to build
 stronger training sets that would yield more explicit results.
\end_layout

\end_body
\end_document
